{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_XiFZTM5LMf",
        "outputId": "e499ebe3-167d-4289-c5c6-84a4e2d06ecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5xMC6bn5kgI",
        "outputId": "9ff9b0d8-5900-40af-8c1c-abf9531d2f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-7meyakc6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-7meyakc6\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4295 sha256=0ca4cbfab94fe69d7320b3f020594fa3597e6ff2677353ad5406ab89be1933be\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cgm96bz3/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7cSsmy25oxE",
        "outputId": "27022fa8-1dc8-4bae-e7b2-fab6c9a0045b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stddef.h>\n",
        "#include <stdint.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define ADJ_MATRIX_ROW_SIZE 1000\n",
        "#define DEBUG 0\n",
        "#define DEBUG_HOST_KER 0\n",
        "#define DEBUG_KER 0\n",
        "#define DEBUG_KER_GRID 0\n",
        "#define PRODUCE_OUT_FILE 1\n",
        "#define ERROR 1\n",
        "#define MAX_FILE_COLUMN_LEN 100\n",
        "#define REAL_NEIGHBOURS_NUM(n) REAL_NODES_NUM(n)\n",
        "#define REAL_NODES_NUM(n) n-1\n",
        "#define STARTING_LEVEL_NUM 1\n",
        "#define STARTING_NODE 0\n",
        "#define USE_HOST 0\n",
        "#define USE_PREFIX_SUM 0\n",
        "\n",
        "#define DEVICE_SHARED_MEM_PER_BLOCK 65536/16\n",
        "\n",
        "const char *file_out_next_level_node_file_name = \"file_out_next_level.txt\";\n",
        "const char *file_out_visited_node_file_name = \"file_out_visited.txt\";\n",
        "//you have to create this directory and load the source test file\n",
        "const char* file_name = \"./tests/standard6.txt\";\n",
        "\n",
        "typedef struct Vector {\n",
        "  int32_t* buff;\n",
        "  int32_t size;\n",
        "} Vector;\n",
        "\n",
        "void print_adj_matrix(Vector* adj_matrix, const int32_t size) {\n",
        "  for (int32_t i=0; i<size; i++) {\n",
        "    printf(\"node: %u size: %u\\n\", i, adj_matrix[i].size);\n",
        "    for (uint32_t j=0; j<adj_matrix[i].size; j++) {\n",
        "      printf(\"%u \", adj_matrix[i].buff[j]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "}\n",
        "\n",
        "uint8_t reallocate(int32_t** ptr, const uint64_t size, int32_t* vec_size) {\n",
        "  *ptr = (int32_t*)realloc((void*)*ptr, size);\n",
        "  if (!ptr) {\n",
        "    return 1;\n",
        "  }\n",
        "  *vec_size = (int32_t)(size / sizeof(int32_t));\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "void adj_to_csr(Vector *adj_matrix, const int32_t size, int32_t *node_ptr,\n",
        "                int32_t **node_neighbours, int32_t *node_neighbours_index,\n",
        "                int32_t *total_neighbours) {\n",
        "  for (int32_t i=0; i<size; i++) {\n",
        "    const int32_t curr_node_len = adj_matrix[i].size;\n",
        "    node_ptr[i] = *node_neighbours_index;\n",
        "    if (*node_neighbours_index >= *total_neighbours) {\n",
        "      uint8_t flag = reallocate(node_neighbours, sizeof(int32_t) * *node_neighbours_index * 2, total_neighbours);\n",
        "      if (flag) {\n",
        "        printf(\"failed to realloc - node_neighbours\\n\");\n",
        "        exit(1);\n",
        "      }\n",
        "    }\n",
        "    if (curr_node_len == 0) {\n",
        "      (*node_neighbours)[*node_neighbours_index] = INT32_MAX;\n",
        "      (*node_neighbours_index)++;\n",
        "    } else {\n",
        "      for (uint32_t j=0; j<curr_node_len; j++) {\n",
        "        if (*node_neighbours_index >= *total_neighbours) {\n",
        "          uint8_t flag = reallocate(node_neighbours, sizeof(int32_t) * *node_neighbours_index * 2, total_neighbours);\n",
        "          if (flag) {\n",
        "            printf(\"failed to realloc - node_neighbours\\n\");\n",
        "            exit(1);\n",
        "          }\n",
        "        }\n",
        "        (*node_neighbours)[*node_neighbours_index] = adj_matrix[i].buff[j];\n",
        "        (*node_neighbours_index)++;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void parse(int32_t* from, int32_t* to, const char* line) {\n",
        "  char from_str[MAX_FILE_COLUMN_LEN] = {0};\n",
        "  char to_str[MAX_FILE_COLUMN_LEN] = {0};\n",
        "  int32_t from_str_index = 0;\n",
        "  int32_t to_str_index = 0;\n",
        "  const int32_t len = strlen(line);\n",
        "  int32_t i = 0;\n",
        "  while (i < len && line[i] != ' ' && line[i] != '\\0' && line[i] != '\\n') {\n",
        "    from_str[from_str_index++] = line[i++];\n",
        "  }\n",
        "  from_str[from_str_index] = '\\0';\n",
        "  i++;\n",
        "  while (i < len && line[i] != ' ' && line[i] != '\\0' && line[i] != '\\n') {\n",
        "    to_str[to_str_index++] = line[i++];\n",
        "  }\n",
        "  *from = atoi(from_str);\n",
        "  *to = atoi(to_str);\n",
        "}\n",
        "\n",
        "void print_result(int32_t* next_level_nodes, const int32_t num_next_level_nodes) {\n",
        "  if (!next_level_nodes) {\n",
        "    printf(\"null node\\n\");\n",
        "  }\n",
        "  for (int32_t i=0; i<num_next_level_nodes; i++) {\n",
        "    printf(\"%u \" ,next_level_nodes[i]);\n",
        "  }\n",
        "  printf(\"\\n\");\n",
        "}\n",
        "\n",
        "#if (USE_HOST == 1)\n",
        "void host_queuing_kernel(int32_t *node_ptr, int32_t *node_neighbours,\n",
        "                         int32_t *node_visited, int32_t *curr_level_nodes,\n",
        "                         int32_t *next_level_nodes,\n",
        "                         const int32_t num_curr_level_nodes,\n",
        "                         const int32_t total_neighbours,\n",
        "                         int32_t *num_next_level_nodes) {\n",
        "  //iterate over neighbours\n",
        "  for (int32_t i=0; i<num_curr_level_nodes; ++i) {\n",
        "    int32_t node = curr_level_nodes[i];\n",
        "#if (DEBUG_HOST_KERNEL == 1)\n",
        "    printf(\"computing node: %u - neighbour index %u\\n\", node, node_ptr[node]);\n",
        "#endif\n",
        "    if (node_neighbours[node_ptr[node]] == INT32_MAX) {\n",
        "      continue;;\n",
        "    }\n",
        "    for (int32_t j=node_ptr[node]; j<node_ptr[node+1]; ++j) {\n",
        "      int32_t n = node_neighbours[j];\n",
        "#if (DEBUG_HOST_KERNEL == 1)\n",
        "      printf(\"j: %u - computing neighbour: %u\\n\", j, n);\n",
        "#endif\n",
        "      if (!node_visited[n]) {\n",
        "        node_visited[n] = 1;\n",
        "        next_level_nodes[*num_next_level_nodes] = n;\n",
        "        (*num_next_level_nodes)++;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "#if (DEBUG == 1)\n",
        "  // print_result(next_level_nodes, *num_next_level_nodes);\n",
        "#endif\n",
        "}\n",
        "#else\n",
        "__global__ void gpu_global_queuing_kernel(int32_t *node_ptr, int32_t *node_neighbours,\n",
        "                         int32_t *node_visited, int32_t *curr_level_nodes,\n",
        "                         int32_t *next_level_nodes,\n",
        "                         const int32_t num_curr_level_nodes,\n",
        "                         const int32_t total_neighbours,\n",
        "                         int32_t *num_next_level_nodes) {\n",
        "  int32_t node_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "#if (DEBUG_KER == 1)\n",
        "  printf(\"node idy %u node idx %u num_curr_level_nodes %u\\n\", node_idx, node_idxx, num_curr_level_nodes);\n",
        "#endif\n",
        "  if (node_idx >= num_curr_level_nodes) {\n",
        "#if (DEBUG_KER == 1)\n",
        "    printf(\"Skipping: %u\\n\", node_idx);\n",
        "#endif\n",
        "    return;\n",
        "  }\n",
        "#if (DEBUG_KER == 1)\n",
        "  else {\n",
        "    printf(\"Computing %u\\n\", node_idx);\n",
        "  }\n",
        "#endif\n",
        "  //iterate over neighbours\n",
        "  int32_t node = curr_level_nodes[node_idx];\n",
        "  if (node_neighbours[node_ptr[node]] == INT32_MAX) {\n",
        "    return;\n",
        "  }\n",
        "  for (int32_t j=node_ptr[node]; j<node_ptr[node+1]; ++j) {\n",
        "    const int32_t n = node_neighbours[j];\n",
        "    const int32_t visited = atomicAnd(&node_visited[n], 1);\n",
        "    if (visited == 0) {\n",
        "      atomicOr(&node_visited[n], 1);\n",
        "      //node_visited[n] = 1;\n",
        "      int32_t next_pos = atomicAdd(num_next_level_nodes, 1);\n",
        "      next_level_nodes[next_pos] = n;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void gpu_block_queuing_kernel(int32_t *node_ptr, int32_t *node_neighbours,\n",
        "                         int32_t *node_visited, int32_t *curr_level_nodes,\n",
        "                         int32_t *next_level_nodes,\n",
        "                         const int32_t num_curr_level_nodes,\n",
        "                         const int32_t total_neighbours,\n",
        "                         int32_t *num_next_level_nodes) {\n",
        "  __shared__ int32_t shared_block_queue[DEVICE_SHARED_MEM_PER_BLOCK]; //max shared mem per multiprocessor / blocks per multiprocessor used (4K)\n",
        "  __shared__ int32_t shared_block_num_next_level_nodes;\n",
        "  shared_block_num_next_level_nodes = 0;\n",
        "  __syncthreads();\n",
        "\n",
        "  int32_t node_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "#if (DEBUG_KER == 1)\n",
        "  printf(\"node idy %u node idx %u num_curr_level_nodes %u\\n\", node_idx, node_idxx, num_curr_level_nodes);\n",
        "#endif\n",
        "  if (node_idx >= num_curr_level_nodes) {\n",
        "#if (DEBUG_KER == 1)\n",
        "    printf(\"Skipping: %u\\n\", node_idx);\n",
        "#endif\n",
        "  } else {\n",
        "    //iterate over neighbours\n",
        "    int32_t node = curr_level_nodes[node_idx];\n",
        "    if (node_neighbours[node_ptr[node]] < INT32_MAX) {\n",
        "      for (int32_t j=node_ptr[node]; j<node_ptr[node+1]; ++j) {\n",
        "        const int32_t n = node_neighbours[j];\n",
        "#if (DEBUG_KER == 1)\n",
        "        printf(\"computing node %u neighbours %u thread %u block %u\\n\", node, n, threadIdx.x, blockIdx.x * blockDim.x);\n",
        "#endif\n",
        "        const int32_t visited = atomicAnd(&node_visited[n], 1);\n",
        "        if (visited == 0) {\n",
        "          atomicOr(&node_visited[n], 1);\n",
        "          int32_t next_pos_shared = atomicAdd(&shared_block_num_next_level_nodes, 1);\n",
        "          shared_block_queue[next_pos_shared] = n;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  __syncthreads();\n",
        "  if (shared_block_num_next_level_nodes < blockDim.x && threadIdx.x < shared_block_num_next_level_nodes) {\n",
        "    int32_t next_pos = atomicAdd(num_next_level_nodes, 1);\n",
        "    next_level_nodes[next_pos] = shared_block_queue[threadIdx.x];\n",
        "  } else if (shared_block_num_next_level_nodes >= blockDim.x && threadIdx.x < blockDim.x) {\n",
        "    const int32_t reminder = shared_block_num_next_level_nodes % blockDim.x;\n",
        "    const int32_t write_per_thread = shared_block_num_next_level_nodes / blockDim.x;\n",
        "    for (int32_t i=0; i<write_per_thread; i++) {\n",
        "      int32_t next_pos = atomicAdd(num_next_level_nodes, 1);\n",
        "      next_level_nodes[next_pos] = shared_block_queue[threadIdx.x * write_per_thread + i];\n",
        "    }\n",
        "    //last thread writes the reminder\n",
        "    if (threadIdx.x == blockDim.x-1) {\n",
        "      for (int32_t i=0; i<reminder; i++) {\n",
        "        int32_t next_pos = atomicAdd(num_next_level_nodes, 1);\n",
        "        next_level_nodes[next_pos] = shared_block_queue[(threadIdx.x+1) * write_per_thread + i];\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "#endif\n",
        "\n",
        "#if (USE_HOST == 1)\n",
        "void launch_host_kernel(int32_t num_curr_level_nodes,\n",
        "                        const int32_t total_neighbours, int32_t *node_ptr,\n",
        "                        int32_t *node_neighbours, int32_t *node_visited,\n",
        "                        int32_t *curr_level_nodes, int32_t *next_level_nodes,\n",
        "                        FILE *out_next_level_nodes) {\n",
        "  double total_elapsed_time = 0.0;\n",
        "  int32_t level = 0;\n",
        "  int8_t done = 0;\n",
        "  int32_t* in_level;\n",
        "  int32_t* out_level;\n",
        "  int32_t num_out_level = num_curr_level_nodes;\n",
        "  while (!done) {\n",
        "    if (level == 0 || level%2 == 0) {\n",
        "      in_level = curr_level_nodes;\n",
        "      out_level = next_level_nodes;\n",
        "    } else {\n",
        "      in_level = next_level_nodes;\n",
        "      out_level = curr_level_nodes;\n",
        "    }\n",
        "    num_curr_level_nodes = num_out_level;\n",
        "    num_out_level = 0;\n",
        "    clock_t start = clock();\n",
        "    host_queuing_kernel(node_ptr, node_neighbours, node_visited, in_level,\n",
        "                        out_level, num_curr_level_nodes, total_neighbours,\n",
        "                        &num_out_level);\n",
        "    clock_t end = clock();\n",
        "    total_elapsed_time += (double)(end - start) * 1000.0 / CLOCKS_PER_SEC;\n",
        "#if (DEBUG == 1)\n",
        "    printf(\"next level nodes: %u:  \", num_out_level);\n",
        "    print_result(out_level, num_out_level);\n",
        "#else\n",
        "    printf(\"next level nodes: %u\\n\", num_out_level);\n",
        "#endif\n",
        "#if (DEBUG == 1 && PRODUCE_OUT_FILE == 1)\n",
        "    if (num_out_level > 0) {\n",
        "      for (int32_t i=0; i<num_out_level; i++) {\n",
        "        fprintf(out_next_level_nodes, \"%u\\n\", out_level[i]);\n",
        "      }\n",
        "      fprintf(out_next_level_nodes, \"-------------\\n\");\n",
        "    }\n",
        "#endif\n",
        "    if (num_out_level == 0) {\n",
        "      done = 1;\n",
        "    }\n",
        "    level++;\n",
        "  }\n",
        "  printf(\"Time elapsed on host queueing: %f ms\\n\", total_elapsed_time);\n",
        "}\n",
        "#else\n",
        "void launch_device_shared_queue_kernel(\n",
        "    const int32_t threads_per_block, int32_t num_curr_level_nodes,\n",
        "    const int32_t total_neighbours, int32_t *d_node_ptr, int32_t *d_node_neighbours,\n",
        "    int32_t *d_node_visited, int32_t *d_curr_level_nodes, int32_t *d_next_level_nodes,\n",
        "    FILE *out_next_level_nodes) {\n",
        "  float total_global_queue_gpu_elapsed_time_ms = 0;\n",
        "  int8_t done = 0;\n",
        "  int32_t levels = 0;\n",
        "  int32_t zero = 0;\n",
        "  int32_t grid_size = 0;\n",
        "  int32_t* in_level = 0;\n",
        "  int32_t* out_level = 0;\n",
        "  int32_t kernel_num_curr_level_nodes = 0;\n",
        "  int32_t* num_out_level = 0;\n",
        "  //initialise num_out_level\n",
        "  cudaMallocManaged((void **) &num_out_level, sizeof(int32_t));\n",
        "  cudaMemcpy(num_out_level, &num_curr_level_nodes, sizeof(int32_t), cudaMemcpyHostToDevice);\n",
        "  while (!done) {\n",
        "    cudaMemcpy(&kernel_num_curr_level_nodes, num_out_level, sizeof(int32_t), cudaMemcpyDeviceToHost);\n",
        "    if (levels == 0 || levels%2 == 0) {\n",
        "      grid_size = (kernel_num_curr_level_nodes + threads_per_block - 1) / threads_per_block;\n",
        "#if (DEBUG_KER_GRID == 1)\n",
        "      printf(\"num grid %u\\n\", grid_size);\n",
        "      printf(\"total threads: %u\\n\", grid_size * threads_per_block);\n",
        "#endif\n",
        "      in_level = d_curr_level_nodes;\n",
        "      out_level = d_next_level_nodes;\n",
        "    } else {\n",
        "      grid_size = (kernel_num_curr_level_nodes + threads_per_block - 1) / threads_per_block;\n",
        "#if (DEBUG_KER_GRID == 1)\n",
        "      printf(\"num grid %u\\n\", grid_size);\n",
        "      printf(\"total threads: %u\\n\", grid_size * threads_per_block);\n",
        "#endif\n",
        "      in_level = d_next_level_nodes;\n",
        "      out_level = d_curr_level_nodes;\n",
        "    }\n",
        "    cudaMemcpy(num_out_level, &zero, sizeof(int32_t), cudaMemcpyHostToDevice);\n",
        "    float level_global_queue_gpu_elapsed_time_ms;\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaEventRecord(start, 0);\n",
        "    gpu_block_queuing_kernel<<<grid_size, threads_per_block>>>(\n",
        "        d_node_ptr, d_node_neighbours, d_node_visited, in_level, out_level,\n",
        "        kernel_num_curr_level_nodes, total_neighbours, num_out_level);\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&level_global_queue_gpu_elapsed_time_ms, start, stop);\n",
        "    total_global_queue_gpu_elapsed_time_ms += level_global_queue_gpu_elapsed_time_ms;\n",
        "\n",
        "    //check stop criteria\n",
        "    int32_t step_nodes;\n",
        "    cudaMemcpy(&step_nodes, num_out_level, sizeof(int32_t), cudaMemcpyDeviceToHost);\n",
        "    int32_t* step_nodes_buff = (int32_t*)malloc(sizeof(int32_t) * step_nodes);\n",
        "    if (step_nodes == 0) {\n",
        "      done = 1;\n",
        "    }\n",
        "#if (DEBUG < 1)\n",
        "    printf(\"next level nodes: %u\\n\", step_nodes);\n",
        "#endif\n",
        "\n",
        "    //debug\n",
        "#if (DEBUG == 1)\n",
        "    if (!step_nodes_buff) {\n",
        "      printf(\"malloc failed - step_nodes_buff - requested size: %u B\\n\", step_nodes * sizeof(uint32_t));\n",
        "    } else if (step_nodes > 0) {\n",
        "      cudaMemcpy(step_nodes_buff, out_level, sizeof(int32_t) * step_nodes, cudaMemcpyDeviceToHost);\n",
        "      printf(\"next level nodes: %u:  \", step_nodes);\n",
        "      print_result(step_nodes_buff, step_nodes);\n",
        "    }\n",
        "#endif\n",
        "#if (DEBUG == 1 && PRODUCE_OUT_FILE == 1)\n",
        "    //save output\n",
        "    if (step_nodes > 0) {\n",
        "      for (int32_t i=0; i<step_nodes; i++) {\n",
        "        fprintf(out_next_level_nodes, \"%u\\n\", step_nodes_buff[i]);\n",
        "      }\n",
        "      fprintf(out_next_level_nodes, \"-------------\\n\");\n",
        "    }\n",
        "#endif\n",
        "    levels++;\n",
        "  }\n",
        "  printf(\"Time elapsed on block GPU queueing: %f ms\\n\",\n",
        "         total_global_queue_gpu_elapsed_time_ms);\n",
        "  cudaFree(num_out_level);\n",
        "}\n",
        "\n",
        "void launch_device_global_queue_kernel(\n",
        "    const int32_t threads_per_block, int32_t num_curr_level_nodes,\n",
        "    const int32_t total_neighbours, int32_t *d_node_ptr, int32_t *d_node_neighbours,\n",
        "    int32_t *d_node_visited, int32_t *d_curr_level_nodes, int32_t *d_next_level_nodes,\n",
        "    FILE *out_next_level_nodes) {\n",
        "  float total_global_queue_gpu_elapsed_time_ms = 0;\n",
        "  int8_t done = 0;\n",
        "  int32_t levels = 0;\n",
        "  int32_t zero = 0;\n",
        "  int32_t grid_size = 0;\n",
        "  int32_t* in_level = 0;\n",
        "  int32_t* out_level = 0;\n",
        "  int32_t kernel_num_curr_level_nodes = 0;\n",
        "  int32_t* num_out_level = 0;\n",
        "  //initialise num_out_level\n",
        "  cudaMallocManaged((void **) &num_out_level, sizeof(int32_t));\n",
        "  cudaMemcpy(num_out_level, &num_curr_level_nodes, sizeof(int32_t), cudaMemcpyHostToDevice);\n",
        "  while (!done) {\n",
        "    cudaMemcpy(&kernel_num_curr_level_nodes, num_out_level, sizeof(int32_t), cudaMemcpyDeviceToHost);\n",
        "    if (levels == 0 || levels%2 == 0) {\n",
        "      grid_size = (kernel_num_curr_level_nodes + threads_per_block - 1) / threads_per_block;\n",
        "#if (DEBUG_KER_GRID == 1)\n",
        "      printf(\"num grid %u\\n\", grid_size);\n",
        "      printf(\"total threads: %u\\n\", grid_size * threads_per_block);\n",
        "#endif\n",
        "      in_level = d_curr_level_nodes;\n",
        "      out_level = d_next_level_nodes;\n",
        "    } else {\n",
        "      grid_size = (kernel_num_curr_level_nodes + threads_per_block - 1) / threads_per_block;\n",
        "#if (DEBUG_KER_GRID == 1)\n",
        "      printf(\"num grid %u\\n\", grid_size);\n",
        "      printf(\"total threads: %u\\n\", grid_size * threads_per_block);\n",
        "#endif\n",
        "      in_level = d_next_level_nodes;\n",
        "      out_level = d_curr_level_nodes;\n",
        "    }\n",
        "    cudaMemcpy(num_out_level, &zero, sizeof(int32_t), cudaMemcpyHostToDevice);\n",
        "    float level_global_queue_gpu_elapsed_time_ms;\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaEventRecord(start, 0);\n",
        "    gpu_global_queuing_kernel<<<grid_size, threads_per_block>>>(\n",
        "        d_node_ptr, d_node_neighbours, d_node_visited, in_level, out_level,\n",
        "        kernel_num_curr_level_nodes, total_neighbours, num_out_level);\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&level_global_queue_gpu_elapsed_time_ms, start, stop);\n",
        "    total_global_queue_gpu_elapsed_time_ms += level_global_queue_gpu_elapsed_time_ms;\n",
        "\n",
        "    //check stop criteria\n",
        "    int32_t step_nodes;\n",
        "    cudaMemcpy(&step_nodes, num_out_level, sizeof(int32_t), cudaMemcpyDeviceToHost);\n",
        "    int32_t* step_nodes_buff = (int32_t*)malloc(sizeof(int32_t) * step_nodes);\n",
        "    if (step_nodes == 0) {\n",
        "      done = 1;\n",
        "    }\n",
        "#if (DEBUG < 1)\n",
        "    printf(\"next level nodes: %u\\n\", step_nodes);\n",
        "#endif\n",
        "\n",
        "    //debug\n",
        "#if (DEBUG == 1)\n",
        "    if (!step_nodes_buff) {\n",
        "      printf(\"malloc failed - step_nodes_buff - requested size: %u B\\n\", step_nodes * sizeof(uint32_t));\n",
        "    } else if (step_nodes > 0) {\n",
        "      cudaMemcpy(step_nodes_buff, out_level, sizeof(int32_t) * step_nodes, cudaMemcpyDeviceToHost);\n",
        "      printf(\"next level nodes: %u:  \", step_nodes);\n",
        "      print_result(step_nodes_buff, step_nodes);\n",
        "    }\n",
        "#endif\n",
        "#if (DEBUG == 1 && PRODUCE_OUT_FILE == 1)\n",
        "    //save output\n",
        "    if (step_nodes > 0) {\n",
        "      for (int32_t i=0; i<step_nodes; i++) {\n",
        "        fprintf(out_next_level_nodes, \"%u\\n\", step_nodes_buff[i]);\n",
        "      }\n",
        "      fprintf(out_next_level_nodes, \"-------------\\n\");\n",
        "    }\n",
        "#endif\n",
        "    levels++;\n",
        "  }\n",
        "  printf(\"Time elapsed on global GPU queueing: %f ms\\n\",\n",
        "         total_global_queue_gpu_elapsed_time_ms);\n",
        "  cudaFree(num_out_level);\n",
        "}\n",
        "#endif\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "#if (USE_HOST == 0)\n",
        "  // retrieve some info abfile_out_next_level_node the CUDA device\n",
        "  int32_t num_devices;\n",
        "  cudaGetDeviceCount(&num_devices);\n",
        "  cudaDeviceProp main_device_prop;\n",
        "  for (int i = 0; i < num_devices; i++) {\n",
        "    cudaDeviceProp prop;\n",
        "    cudaGetDeviceProperties(&prop, i);\n",
        "    memcpy(&main_device_prop, &prop, sizeof(cudaDeviceProp));\n",
        "    printf(\"Device Number: %d\\n\", i);\n",
        "    printf(\"  Device name: %s\\n\", prop.name);\n",
        "    printf(\"  max Blocks Per MultiProcessor: %d\\n\", prop.maxBlocksPerMultiProcessor);\n",
        "    printf(\"  max Threads Per MultiProcessor: %d\\n\", prop.maxThreadsPerMultiProcessor);\n",
        "    printf(\"  max Threads Per Block: %d\\n\", prop.maxThreadsPerBlock);\n",
        "    printf(\"  num SM: %d\\n\", prop.multiProcessorCount);\n",
        "    printf(\"  num bytes sharedMem Per Block: %d\\n\", prop.sharedMemPerBlock);\n",
        "    printf(\"  num bytes sharedMem Per Multiprocessor: %d\\n\", prop.sharedMemPerMultiprocessor);\n",
        "    printf(\"  Memory Clock Rate (KHz): %d\\n\",\n",
        "         prop.memoryClockRate);\n",
        "    printf(\"  Memory Bus Width (bits): %d\\n\",\n",
        "         prop.memoryBusWidth);\n",
        "    printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\",\n",
        "         2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
        "  }\n",
        "#endif\n",
        "  //device params\n",
        "  int32_t* d_node_ptr = 0;\n",
        "  int32_t* d_node_neighbours = 0;\n",
        "  int32_t* d_node_visited = 0;\n",
        "  int32_t* d_curr_level_nodes = 0;\n",
        "  int32_t* d_next_level_nodes = 0;\n",
        "#if (USE_HOST == 0)\n",
        "  int32_t* num_next_level_nodes = 0;\n",
        "  cudaMallocManaged((void **) &num_next_level_nodes, sizeof(int32_t));\n",
        "#endif\n",
        "\n",
        "  //host params\n",
        "  int32_t max_node_val = 0;\n",
        "  int32_t line_counter = 0;\n",
        "  int32_t num_nodes = 0;\n",
        "  int32_t total_neighbours = 0;\n",
        "  int32_t* node_ptr = 0;\n",
        "  int32_t* node_neighbours = 0;\n",
        "  int32_t* node_visited = 0;\n",
        "  int32_t* curr_level_nodes = 0;\n",
        "  int32_t* next_level_nodes = 0;\n",
        "  int32_t num_curr_level_nodes = 0;\n",
        "  Vector* adj_matrix = 0;\n",
        "#if (USE_HOST == 1)\n",
        "  int32_t num_next_level_nodes = 0;\n",
        "#endif\n",
        "  FILE *file;\n",
        "  int32_t node_ptr_index = 0;\n",
        "  int32_t node_neighbours_index = 0;\n",
        "  int32_t last_node_val = 0;\n",
        "  FILE *file_out_next_level_node;\n",
        "  FILE *file_out_visited_node;\n",
        "  file = fopen(file_name, \"r\");\n",
        "  if (file == NULL) {\n",
        "    printf(\"Error opening the file.\\n\");\n",
        "    return 1;\n",
        "  }\n",
        "  file_out_next_level_node = fopen(file_out_next_level_node_file_name, \"w\");\n",
        "  if (file_out_next_level_node == NULL) {\n",
        "    printf(\"Error opening the file_out_next_level_node file.\\n\");\n",
        "    return 1;\n",
        "  }\n",
        "  file_out_visited_node = fopen(file_out_visited_node_file_name, \"w\");\n",
        "  if (file_out_visited_node == NULL) {\n",
        "    printf(\"Error opening the file_out_next_level_node file.\\n\");\n",
        "    return 1;\n",
        "  }\n",
        "\n",
        "  char line[MAX_FILE_COLUMN_LEN] = {0};\n",
        "  while (fgets(line, sizeof(line), file) != NULL) {\n",
        "    if (!line_counter) {\n",
        "      (line_counter)++;\n",
        "      num_nodes = atoi(line);\n",
        "    }\n",
        "    //counting the first line, yes one too much but then compensated with the virtual node added\n",
        "    (total_neighbours)++;\n",
        "  }\n",
        "  fclose(file);\n",
        "  //add virtual node to avoid index overflow in the computation later\n",
        "  (num_nodes)++;\n",
        "  //heuristic initial allocation\n",
        "  total_neighbours += 2 * num_nodes;\n",
        "  printf(\"total nodes: %d\\n\", REAL_NODES_NUM(num_nodes));\n",
        "\n",
        "  //allocate buffers\n",
        "  node_ptr = (int32_t*)malloc(sizeof(int32_t) * num_nodes);\n",
        "  if (!node_ptr) {\n",
        "    printf(\"failed malloc - node_ptr\\n\");\n",
        "    return 1;\n",
        "  }\n",
        "  for (int32_t i=0; i<num_nodes; i++) {\n",
        "    node_ptr[i] = INT32_MAX;\n",
        "  }\n",
        "#if (USE_HOST == 0)\n",
        "  cudaMallocManaged((void **) &d_node_ptr, sizeof(int32_t) * num_nodes);\n",
        "  if (!d_node_ptr) {\n",
        "    printf(\"failed malloc - d_node_ptr\\n\");\n",
        "  }\n",
        "#endif\n",
        "  curr_level_nodes = (int32_t*)malloc(sizeof(int32_t) * num_nodes);\n",
        "#if (USE_HOST == 0)\n",
        "  cudaMallocManaged((void **) &d_curr_level_nodes, sizeof(int32_t) * num_nodes);\n",
        "  if (!d_curr_level_nodes) {\n",
        "    printf(\"failed malloc - d_curr_level_nodes\\n\");\n",
        "  }\n",
        "#endif\n",
        "  if (!curr_level_nodes) {\n",
        "    printf(\"failed malloc - curr_level_nodes\\n\");\n",
        "  }\n",
        "  node_neighbours = (int32_t*)malloc(sizeof(int32_t) * total_neighbours);\n",
        "  //cuda allocation happens later\n",
        "  if (!node_neighbours) {\n",
        "    printf(\"failed malloc - node_neighbours\\n\");\n",
        "  }\n",
        "  next_level_nodes = (int32_t*)malloc(sizeof(int32_t) * total_neighbours);\n",
        "#if (USE_HOST == 0)\n",
        "  cudaMallocManaged((void **) &d_next_level_nodes, sizeof(int32_t) * total_neighbours);\n",
        "  if (!d_next_level_nodes) {\n",
        "    printf(\"failed malloc - d_next_level_nodes\\n\");\n",
        "  }\n",
        "#endif\n",
        "  if (!next_level_nodes) {\n",
        "    printf(\"failed malloc - next_level_nodes\\n\");\n",
        "  }\n",
        "\n",
        "  file = fopen(file_name, \"r\");\n",
        "  line_counter = 0;\n",
        "  if (file == NULL) {\n",
        "    printf(\"Can not open the file\");\n",
        "    return 1;\n",
        "  }\n",
        "\n",
        "  adj_matrix = (Vector*)malloc(sizeof(Vector) * num_nodes);\n",
        "  if (!adj_matrix) {\n",
        "    printf(\"malloc failed - adj_matrix\\n\");\n",
        "    return 1;\n",
        "  }\n",
        "  for (int32_t i=0; i<num_nodes; i++) {\n",
        "    adj_matrix[i].buff = (int32_t*)malloc(sizeof(int32_t) * ADJ_MATRIX_ROW_SIZE);\n",
        "    if (!adj_matrix[i].buff) {\n",
        "      printf(\"malloc failed - adj_matrix[%u]\\n\", i);\n",
        "      return 1;\n",
        "    }\n",
        "    adj_matrix[i].size = 0;\n",
        "  }\n",
        "  while (fgets(line, sizeof(line), file) != NULL) {\n",
        "    if (!line_counter) {\n",
        "      (line_counter)++;\n",
        "      continue;\n",
        "    }\n",
        "    int32_t from;\n",
        "    int32_t to;\n",
        "    parse(&from, &to, line);\n",
        "    adj_matrix[from].buff[adj_matrix[from].size++] = to;\n",
        "    adj_matrix[to].buff[adj_matrix[to].size++] = from;\n",
        "  }\n",
        "  fclose(file);\n",
        "\n",
        "  //print adj matrix\n",
        "#if (DEBUG > 2)\n",
        "  print_adj_matrix(adj_matrix, num_nodes);\n",
        "#endif\n",
        "\n",
        "  //move\n",
        "  adj_to_csr(adj_matrix, num_nodes, node_ptr, &node_neighbours, &node_neighbours_index, &total_neighbours);\n",
        "#if (USE_HOST == 0)\n",
        "  cudaMallocManaged((void **) &d_node_neighbours, sizeof(int32_t) * total_neighbours);\n",
        "  if (!d_node_neighbours) {\n",
        "    printf(\"failed malloc - node_neighbours\\n\");\n",
        "  }\n",
        "#endif\n",
        "  total_neighbours = node_neighbours_index;\n",
        "\n",
        "  max_node_val = num_nodes;\n",
        "  node_visited = (int32_t*)malloc(sizeof(int32_t) * max_node_val);\n",
        "  if (!node_visited) {\n",
        "    printf(\"failed malloc - node_visited)\\n\");\n",
        "  }\n",
        "  memset(node_visited, 0, sizeof(int32_t) * max_node_val);\n",
        "#if (USE_HOST == 0)\n",
        "  cudaMallocManaged((void **) &d_node_visited, sizeof(int32_t) * max_node_val);\n",
        "  if (!d_node_visited) {\n",
        "    printf(\"failed malloc - d_node_visited)\\n\");\n",
        "  }\n",
        "#endif\n",
        "\n",
        "#if (DEBUG > 1)\n",
        "  printf(\"node ptr:\\n\");\n",
        "  for (int32_t i=0; i<num_nodes; i++) {\n",
        "    printf(\"%u \", (node_ptr)[i]);\n",
        "  }\n",
        "  printf(\"\\n\");\n",
        "  printf(\"node neighbours:\\n\");\n",
        "  for (int32_t i=0; i<total_neighbours; i++) {\n",
        "    printf(\"%u \", (node_neighbours)[i]);\n",
        "  }\n",
        "  printf(\"\\n\");\n",
        "#endif\n",
        "\n",
        "#if (USE_HOST == 0 && USE_PREFIX_SUM == 1)\n",
        "#error \"Prefix sum not supported\"\n",
        "  //yet not supported\n",
        "  //compute prefix sum\n",
        "  int32_t* prefix_sum = 0;\n",
        "  cudaMallocManaged((void **) &prefix_sum, sizeof(int32_t) * num_nodes);\n",
        "  int32_t count = 0;\n",
        "  for (int32_t i=0; i<num_nodes-1; i++) {\n",
        "    prefix_sum[i] = count;\n",
        "    count += node_ptr[i+1] - node_ptr[i];\n",
        "  }\n",
        "  prefix_sum[num_nodes-1] = count;\n",
        "  printf(\"prefix sum\\n\");\n",
        "  for (int32_t i=0; i<num_nodes; i++) {\n",
        "    printf(\"%u \", prefix_sum[i]);\n",
        "  }\n",
        "  printf(\"\\n\");\n",
        "  //assign next level nodes count\n",
        "  *num_next_level_nodes = count;\n",
        "#endif\n",
        "\n",
        "#if (USE_HOST == 1)\n",
        "  //populate starting curr_level_nodes\n",
        "  curr_level_nodes[0] = STARTING_NODE;\n",
        "  num_curr_level_nodes = STARTING_LEVEL_NUM;\n",
        "  node_visited[0] = 1;\n",
        "  printf(\"\\nlaunching host kernel with initial level nodes size: %u\\n\", num_curr_level_nodes);\n",
        "  launch_host_kernel(num_curr_level_nodes, total_neighbours, node_ptr,\n",
        "                     node_neighbours, node_visited, curr_level_nodes,\n",
        "                     next_level_nodes, file_out_next_level_node);\n",
        "#else\n",
        "  //populate starting curr_level_nodes\n",
        "  curr_level_nodes[0] = STARTING_NODE;\n",
        "  num_curr_level_nodes = STARTING_LEVEL_NUM;\n",
        "  node_visited[0] = 1;\n",
        "  cudaMemcpy(d_node_visited, node_visited, sizeof(int32_t), cudaMemcpyHostToDevice); //copy first 4 bytes\n",
        "  printf(\"\\nlaunching device kernel (global queue) with initial level nodes size: %u\\n\", num_curr_level_nodes);\n",
        "  //copy resources\n",
        "  cudaMemcpy(d_node_neighbours, node_neighbours, total_neighbours * sizeof(int32_t), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_node_ptr, node_ptr, num_nodes * sizeof(int32_t), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_curr_level_nodes, curr_level_nodes, num_nodes * sizeof(int32_t), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_node_visited, node_visited, max_node_val * sizeof(int32_t), cudaMemcpyHostToDevice);\n",
        "\n",
        "  const int32_t threads_per_block = main_device_prop.maxThreadsPerBlock/main_device_prop.maxBlocksPerMultiProcessor;\n",
        "\n",
        "  launch_device_global_queue_kernel(threads_per_block, num_curr_level_nodes, total_neighbours, d_node_ptr,\n",
        "                     d_node_neighbours, d_node_visited, d_curr_level_nodes,\n",
        "                     d_next_level_nodes, file_out_next_level_node);\n",
        "\n",
        "  //copy resources\n",
        "  cudaMemcpy(d_node_neighbours, node_neighbours, total_neighbours * sizeof(int32_t), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_node_ptr, node_ptr, num_nodes * sizeof(int32_t), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_curr_level_nodes, curr_level_nodes, num_nodes * sizeof(int32_t), cudaMemcpyHostToDevice);\n",
        "  memset(node_visited, 0, sizeof(int32_t) * max_node_val);\n",
        "  cudaMemcpy(d_node_visited, node_visited, max_node_val * sizeof(int32_t), cudaMemcpyHostToDevice);\n",
        "  //populate starting curr_level_nodes\n",
        "  curr_level_nodes[0] = STARTING_NODE;\n",
        "  num_curr_level_nodes = STARTING_LEVEL_NUM;\n",
        "  node_visited[0] = 1;\n",
        "  cudaMemcpy(d_node_visited, node_visited, sizeof(int32_t), cudaMemcpyHostToDevice); //copy first 4 bytes\n",
        "  printf(\"\\nlaunching device kernel (shared queue) with initial level nodes size: %u\\n\", num_curr_level_nodes);\n",
        "\n",
        "  launch_device_shared_queue_kernel(threads_per_block, num_curr_level_nodes, total_neighbours, d_node_ptr,\n",
        "                     d_node_neighbours, d_node_visited, d_curr_level_nodes,\n",
        "                     d_next_level_nodes, file_out_next_level_node);\n",
        "#endif\n",
        "\n",
        "#if (USE_HOST == 0)\n",
        "  cudaMemcpy(node_visited, d_node_visited, max_node_val * sizeof(int32_t), cudaMemcpyDeviceToHost);\n",
        "  for (int32_t i=0; i<max_node_val; i++) {\n",
        "    fprintf(file_out_visited_node, \"%u\\n\", node_visited[i]);\n",
        "  }\n",
        "#else\n",
        "  for (int32_t i=0; i<max_node_val; i++) {\n",
        "    fprintf(file_out_visited_node, \"%u\\n\", node_visited[i]);\n",
        "  }\n",
        "#endif\n",
        "  fclose(file_out_next_level_node);\n",
        "  fclose(file_out_visited_node);\n",
        "\n",
        "  free(node_ptr);\n",
        "  free(node_neighbours);\n",
        "  free(curr_level_nodes);\n",
        "  free(next_level_nodes);\n",
        "  free(node_visited);\n",
        "  for (int32_t i=0; i<num_nodes; i++) {\n",
        "    if (adj_matrix[i].buff) {\n",
        "      free(adj_matrix[i].buff);\n",
        "    }\n",
        "  }\n",
        "  free(adj_matrix);\n",
        "#if (USE_HOST == 0)\n",
        "  cudaFree(d_node_ptr);\n",
        "  cudaFree(d_node_neighbours);\n",
        "  cudaFree(d_curr_level_nodes);\n",
        "  cudaFree(d_next_level_nodes);\n",
        "  cudaFree(d_node_visited);\n",
        "  cudaFree(num_next_level_nodes);\n",
        "#endif\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukAFZT3-5rJR",
        "outputId": "03e25ca9-2dc0-4559-d0eb-569a7c852535"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device Number: 0\n",
            "  Device name: Tesla T4\n",
            "  max Blocks Per MultiProcessor: 16\n",
            "  max Threads Per MultiProcessor: 1024\n",
            "  max Threads Per Block: 1024\n",
            "  num SM: 40\n",
            "  num bytes sharedMem Per Block: 49152\n",
            "  num bytes sharedMem Per Multiprocessor: 65536\n",
            "  Memory Clock Rate (KHz): 5001000\n",
            "  Memory Bus Width (bits): 256\n",
            "  Peak Memory Bandwidth (GB/s): 320.064000\n",
            "\n",
            "total nodes: 2000000\n",
            "\n",
            "launching device kernel (global queue) with initial level nodes size: 1\n",
            "next level nodes: 3\n",
            "next level nodes: 9\n",
            "next level nodes: 36\n",
            "next level nodes: 128\n",
            "next level nodes: 447\n",
            "next level nodes: 1521\n",
            "next level nodes: 5174\n",
            "next level nodes: 17512\n",
            "next level nodes: 58483\n",
            "next level nodes: 186555\n",
            "next level nodes: 498699\n",
            "next level nodes: 809453\n",
            "next level nodes: 407621\n",
            "next level nodes: 25431\n",
            "next level nodes: 56\n",
            "next level nodes: 0\n",
            "Time elapsed on global GPU queueing: 20.931198 ms\n",
            "\n",
            "launching device kernel (shared queue) with initial level nodes size: 1\n",
            "next level nodes: 3\n",
            "next level nodes: 9\n",
            "next level nodes: 36\n",
            "next level nodes: 128\n",
            "next level nodes: 447\n",
            "next level nodes: 1521\n",
            "next level nodes: 5174\n",
            "next level nodes: 17510\n",
            "next level nodes: 58350\n",
            "next level nodes: 185471\n",
            "next level nodes: 495874\n",
            "next level nodes: 805887\n",
            "next level nodes: 405481\n",
            "next level nodes: 25102\n",
            "next level nodes: 40\n",
            "next level nodes: 0\n",
            "Time elapsed on block GPU queueing: 11.615617 ms\n",
            "\n"
          ]
        }
      ]
    }
  ]
}